# Principal Component Analysis

Principal Component Analysis (PCA) is a dimensionality reduction techinque used in machine learning and data analysis. It aims to transfom a high-dimensional dataset into lower-dimensional space while retaining as much of the original information as possible.

## What PCA doesS?
1) **Reduces Dimensionality:** PCA takes a dataset with a large number of variables(features) and reduced it to a smaller set of variables, known as principal components. These components are linear combinations of the original variable.<br>
2) **Retains Information:** The principal components are constructed in such a way that the first component accounts for the maximum possible variance in the data, the second component(orthogonal to the first) accounts for the next higher variance, and so on. This means that even though we're reducing the number of dimensions, we're trying to retain as much information as possible.<br>
3) **Uncorrelated Components:** The principal components are orthogonal to each other. This means they are uncorrelated, which can be a very useful property in various applications.<br>
4) **Basis for Transformations:** The first principal component is the direction in which the data varies the most. The second principal component is the direction that accounts for the most remaining variance, orthogonal to the first. In this way, each subsequent principal component capture less and less of the remaining variance.<br>
5) **Eigenvalues and EigenvectorsL:** PCA involves finding the eigenvalues and eigenvectors of the data covariance matrix. These values tell us about the variance in different directions in the original feature space. The eigenvectors form the new basis, and the eigenvalues tell us how much variance is captured in each component.<br>
6) **Data Compression:** PCA can be used to compress data. By keeping only the most important components, you can represent the data with fewer numbers, which can be beneficial in storage and processing.<br>
7) **Noise Reduction:** PCA can help in reducing noise in data. Since the lower-variance components are often associated with noise or less important variations in the data, discarding them can lead to a cleaner representation of the data.<br>
8) **Visualization:**  PCA is often used for visualizing high-dimensional data. By projecting it onto a lower-dimensional space (usually 2D or 3D), we can plot the data and gain insights into its underlying structure.<br>
9) **Feature Engineering:** PCA can be used as a feature engineering technique. Instead of using all the original features, we can use a reduced set of principal components which can sometimes lead to better performance in machine learning models.<br>
10) **Independence from Units:** PCA is scale-invariant, meaning it's not affected by the scale of the variables. This can be particularly useful when dealing with data where the units of measurement are not consistent across features.<br>

Overall, PCA is a powerful tool for exploratory data analysis, visualization, and preparing data for further analysis or modeling. It helps to reveal the underlying structure of complex datasets, making it easier to extract meaningful insights.